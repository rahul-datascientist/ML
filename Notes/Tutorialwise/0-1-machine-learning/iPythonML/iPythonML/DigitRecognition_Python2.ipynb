{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The objective is to train a multi-layered neural network to \n",
    "# identify hand written digits. \n",
    "\n",
    "# We'll be using a database called MNIST which has about 60000 handwritten \n",
    "# digit images with their labels. We'll use these to train a network. Then\n",
    "# given a new image, the network should be able to classify it as the right digit \n",
    "# between 0-9 \n",
    "import numpy as np\n",
    "import os \n",
    "# Step 1: We'll download the database of images from MNIST website - this is maintained\n",
    "# by a famous Neural network researcher named Yann Lecun\n",
    "def load_dataset():\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print (\"Downloading \",filename)\n",
    "        import urllib\n",
    "        urllib.urlretrieve(source+filename,filename)\n",
    "    # This will download the specified file from Yann Lecun's website and store it \n",
    "    # on our local disk\n",
    "    \n",
    "    import gzip\n",
    "    \n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Checks if the specified file is already there on our local disk\n",
    "        # if not it will download the file \n",
    "        with gzip.open(filename,'rb') as f:\n",
    "            # Open the zip file of images \n",
    "            data=np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "            # This is some boilerplate to extract data from the zip file\n",
    "            # This data has 2 issues : its in the form of a 1 d array\n",
    "            # We have to take this array and convert it into images \n",
    "            # Each image has 28x28 pixels , its a monochrome image ie only 1 channel (if\n",
    "            # it were full-color it would have 3/4 channels R,G,B etc)\n",
    "            \n",
    "            # data is currently a numpy array which we we want to reshape into \n",
    "            # an array of 28x28 images \n",
    "            data=data.reshape(-1,1,28,28)\n",
    "            # The first dimension is the number of images , by making this -1\n",
    "            # The number of images will be inferred from the value of the other dimensions\n",
    "            # and the length of the input array\n",
    "            \n",
    "            # The second dimension is the number of channels - here this is 1\n",
    "            \n",
    "            # The third and fourth dimensions are the size of the image 28x28\n",
    "            \n",
    "            # its in the form of bytes \n",
    "            return data/np.float32(256)\n",
    "        # This will convert the byte value to a float32 in the range [0,1]\n",
    "        \n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "                # Read the labels which are in a binary file again \n",
    "        with gzip.open(filename,'rb') as f:\n",
    "            data = np.frombuffer(f.read(),np.uint8,offset=8)\n",
    "                    # This gives a numpy array of integers, the digit value corresponding \n",
    "                    # to the images we got earlier \n",
    "        return data\n",
    "    # We can now download and read the training and test data sets - both the images \n",
    "    # and their labels \n",
    "    \n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WE've got our data ready now. If you want to look at one of the images \n",
    "# you can use matplotlib \n",
    "import matplotlib \n",
    "matplotlib.use('TkAgg') # This is just a default setting for matplotlib for how \n",
    "# to render images \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.show(plt.imshow(X_train[3][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: We'll set up a neural network with the required number of layers and nodes \n",
    "# We'll also tell the network how it has to train itself \n",
    "\n",
    "# We are going to use 2 Python packages called Theano and Lasagne \n",
    "# Theano is a mathematical package that allows you to define and perform \n",
    "# mathematical computations. - like numpy but with high dimensional arrays\n",
    "# Higher dimensional arrays are often called Tensors - and Theano is a python \n",
    "# package to work with them \n",
    "\n",
    "# Lasagne is a library that uses Theano heavily and supports building of \n",
    "# neural networks. It comes with functions to set up layers , define error functions \n",
    "# train neural networks etc \n",
    "\n",
    "# Make sure you install the latest versions of Lasagne and Theano - you can find \n",
    "# the install command on the corresponding github pages. \n",
    "\n",
    "# WE forgot to import the required libraries :) \n",
    "import lasagne \n",
    "\n",
    "import theano\n",
    "import theano.tensor as T \n",
    "\n",
    "def build_NN(input_var=None):\n",
    "    # WE are going to create a neural network with 2 hidden layers of 800 nodes each \n",
    "    # The output layer will have 10 nodes - the nodes are numbered 0-9 and the output \n",
    "    # at each node will be a value between 0-1. The node with the highest value will \n",
    "    # be the predicted output\n",
    "    \n",
    "    # First we have an input layer - the expected input shape is \n",
    "    # 1x28x28 (for 1 image)\n",
    "    # We will link this input layer to the input_var (which will be the array of images \n",
    "    # that we'll pass in later on)\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None,1,28,28),input_var=input_var)\n",
    "    \n",
    "    # We'll add a 20% dropout - this means that randomly 20% of the edges between the \n",
    "    # inputs and the next layer will be dropped - this is done to avoid overfitting \n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in,p=0.2)\n",
    "    \n",
    "    # Add a layer with 800 nodes. Initially this will be dense/fully-connected\n",
    "    # ie. every edge possible \n",
    "    # will be drawn. \n",
    "    l_hid1= lasagne.layers.DenseLayer(l_in_drop,num_units=800,\n",
    "                                      nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                      W=lasagne.init.GlorotUniform())\n",
    "    # This layer has been initialized with some weights. There are some schemes to \n",
    "    # initialize the weights so that training will be done faster, Glorot's scheme\n",
    "    # is one of them \n",
    "    \n",
    "    # We will add a dropout of 50% to the hidden layer 1 \n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1,p=0.5)\n",
    "    \n",
    "    # Add another layer, it works exactly the same way! \n",
    "    l_hid2= lasagne.layers.DenseLayer(l_hid1_drop,num_units=800,\n",
    "                                      nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                      W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2,p=0.5)\n",
    "    \n",
    "    \n",
    "    # Let's now add the final output layer. \n",
    "    l_out = lasagne.layers.DenseLayer(l_hid2_drop, num_units=10,\n",
    "                                     nonlinearity = lasagne.nonlinearities.softmax)\n",
    "    # the output layer has 10 units. Softmax specifies that each of those \n",
    "    # outputs is between 0-1 and the max of the those will be the final prediction \n",
    "    \n",
    "    return l_out # We return the last layer, but since all the layers are linked \n",
    "# we effectively return the whole network \n",
    "    \n",
    "    \n",
    "# We've set up the network. Now we have to tell the network how to train itself \n",
    "# ie how should it find the values of all the weights it needs to find \n",
    "\n",
    "# We'll initialize some empty arrays which will act as placeholders \n",
    "# for the training/test data that will be given to the network \n",
    "\n",
    "input_var = T.tensor4('inputs') # An empty 4 dimensional array \n",
    "target_var = T.ivector('targets') # An empty 1 dimensional integer array to represent\n",
    "# the labels \n",
    "\n",
    "network=build_NN(input_var) # Call the function that initializes the neural network \n",
    "\n",
    "# In training we are going to follow the steps below \n",
    " # a. compute an error function \n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "# Categorical cross entropy is one of the standard error functions with \n",
    "# classification problems \n",
    "loss = loss.mean()\n",
    "\n",
    "# b. We'll tell the network how to update all its weights based on the \n",
    "# value of the error function \n",
    "params = lasagne.layers.get_all_params(network, trainable=True) # Current value of all the weights \n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01, momentum = 0.9)\n",
    "\n",
    "# Nesterov momentum is one of the options that Lasagne offers for updating the weights \n",
    "# in a training step . This is based on Stochastic Gradient Descent - the idea is simple\n",
    "# Find the slope of the error function at the current point and move downwards \n",
    "# in the direction of that slope \n",
    "\n",
    "# We'll use theano to compile a function that is going to represent a \n",
    "# single training step ie. compute the error, find the current weights, update the weights\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "# calling this function for a certain number of times will train the neural network \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step is 0\n",
      "Current step is 1\n",
      "Current step is 2\n",
      "Current step is 3\n",
      "Current step is 4\n",
      "Current step is 5\n",
      "Current step is 6\n",
      "Current step is 7\n",
      "Current step is 8\n",
      "Current step is 9\n"
     ]
    }
   ],
   "source": [
    "# Step 3: We'll feed the training data to the neural network \n",
    "num_training_steps = 10 # ideally you can train for a few 100 steps\n",
    "\n",
    "for step in range(num_training_steps):\n",
    "    train_err = train_fn(X_train, y_train)\n",
    "    print(\"Current step is \"+ str(step))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12529882,  0.03095455,  0.10531944,  0.08858692,  0.06188799,\n",
       "         0.08941018,  0.09295384,  0.1472574 ,  0.11782097,  0.14050988]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: We'll check how the output is for 1 image \n",
    "# To check the prediction for 1 image we'll need to set up another function \n",
    "test_prediction = lasagne.layers.get_output(network)\n",
    "val_fn = theano.function([input_var],test_prediction)\n",
    "\n",
    "val_fn([X_test[0]]) # This will apply the function on 1 image, the first one in the test set\n",
    "\n",
    "# The max value if for the digit 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the actual value \n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.585)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5 : We'll feed a test data set of 10000 images to the trained neural network\n",
    "# and check it's accuracy \n",
    "\n",
    "# We'll set up a function that will take in images and their labels , feed the images \n",
    "# to our network and compute it's accuracy \n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network,deterministic=True)\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1),target_var),dtype=theano.config.floatX)\n",
    "# Checks the index of the max value in each test prediction and matches it agains the actual \n",
    "# value \n",
    "\n",
    "acc_fn = theano.function([input_var,target_var],test_acc)\n",
    "\n",
    "acc_fn(X_test,y_test)\n",
    "# This is pretty poor accuracy - but to improve it you can run the training for more \n",
    "# number of steps. You could also divide the training set arbitrarily into smaller datasets\n",
    "# and run each of the training set on that smaller dataset. This will run faster \n",
    "# and take you to the error minimum faster - while avoiding overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
