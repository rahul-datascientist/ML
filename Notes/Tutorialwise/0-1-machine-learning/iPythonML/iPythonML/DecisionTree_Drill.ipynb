{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a passenger on the Titanic and their attributes - like age, sex, socio-economic statusl the objective is to \n",
    "# predict whether they would have survived or not \n",
    "\n",
    "# This is one of the challenges on Kaggle - a competitive online Data Science community\n",
    "# We'll fetch the data from the Kaggle website and in the end we'll submit our results to see where we stand\n",
    "# among the Data Science heavyweights :) \n",
    "\n",
    "# The csv contains the following details \n",
    "# passengerid      The row/serial number \n",
    "# survival         Survival (0 = No; 1= Yes)\n",
    "# pclass           Passenger Class(1 = 1st; 2 = 2nd; 3=3rd) This can be a proxy for socio-economic status\n",
    "# name             Name\n",
    "# sex              Sex (male/female) This will need to be converted to a numerical vairable. More on this later\n",
    "# age              Age (in years); Fractional if age<1 and represented as xx.5 if its an estimated age\n",
    "# sibsp            Number of Siblings/Spouses Aboard\n",
    "# parch            Number of Parents/Children Aboard\n",
    "# ticket           Ticket Number \n",
    "# fare             Passenger Fare\n",
    "# cabin            Cabin \n",
    "# embarked         Port of Embarkation (C=Cherbourg; Q=Queenstown; S=Southampton)\n",
    "\n",
    "# We'll use sci-kit learn's DecisionTreeClassifier. This classifier takes only numeric variables as part of the feature \n",
    "# vector. We will have to convert the categorical variables like Sex and Port of Embarkation to numeric variables\n",
    "# WE'll do this by mapping each of the labels to a number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "def transformDataTitanic(trainingFile, features):\n",
    "    # This function will read the data in the training file and transform it \n",
    "    # into a list of lists. \n",
    "    # Let's initialize a variable to hold this list of lists\n",
    "    transformData=[]\n",
    "    # The function will also return a list with the labels (Survived (0/1)) for\n",
    "    # each of the passengers\n",
    "    labels = []\n",
    "    # Now we'll set up a couple of maps. These will be used to convert the \n",
    "    # categorical variables like gender to numeric variables \n",
    "    \n",
    "    genderMap = {\"male\":1,\"female\":2,\"\":\"\"} # We include a key in the map for missing values\n",
    "    embarkMap = {\"C\":1,\"Q\":2,\"S\":3,\"\":\"\"}\n",
    "    # Notice how the map contains a key for blanks. The csv we got has a lot of missing values\n",
    "    # and how you deal with these could have a big impact on your ability to predict accurately. \n",
    "    # For now we are just going to ignore any passengers who have missing values in any of the features\n",
    "    # We'll initialize a blank string to perform this check before processing a passenger\n",
    "    blank=\"\"\n",
    "    # Now we are finally ready to read the csv file\n",
    "    with open(trainingFile,'r') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        lineNum=1\n",
    "        # lineNum will help us keep track of which row we are in \n",
    "        for row in lineReader:\n",
    "            if lineNum==1:\n",
    "                # if it's the first row, just store the names in the header in a list. The features\n",
    "                # That are passed in to our function will be a subset of this list\n",
    "                # PassengerID, Survived, PClass, Name, Sex, Age, Sibsp, Parch, Ticket, Fare, Cabin,\n",
    "                # Embarked\n",
    "                header = row\n",
    "            else: \n",
    "                allFeatures=list(map(lambda x:genderMap[x] if row.index(x)==4\n",
    "                               else embarkMap[x] if row.index(x)==11 else x, row))\n",
    "                # allFeatures is a list where we have converted the categorical variables to \n",
    "                # numerical variables\n",
    "                featureVector = [allFeatures[header.index(feature)] for feature in features]\n",
    "                # featureVector is a subset of allFeatures, it contains only those features\n",
    "                # that are specified by us in the function argument\n",
    "                if blank not in featureVector:\n",
    "                    transformData.append(featureVector)\n",
    "                    labels.append(int(row[1]))\n",
    "                    # if the featureVector contains missing values, skip it, else add the featureVector\n",
    "                    # to our transformedData and the corresponding label to the list of labels\n",
    "            lineNum=lineNum+1\n",
    "        return transformData,labels\n",
    "    # return both our list of feature vectors and the list of labels \n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take this for a spin now\n",
    "trainingFile=\"/Users/swethakolalapudi/Desktop/Kaggle/Titanic/train.csv\"\n",
    "features=[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Fare\",\"Parch\",\"Embarked\"]\n",
    "trainingData=transformDataTitanic(trainingFile,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are now ready to train our Decision Tree classifier\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "clf=tree.DecisionTreeClassifier(max_leaf_nodes=20)\n",
    "X=np.array(trainingData[0])\n",
    "y=np.array(trainingData[1])\n",
    "clf=clf.fit(X,y)\n",
    "# The fit method takes in 2 numpy arrays, one with the feature vectors and the other with the \n",
    "# labels. It uses a modified version of the CART algorithm and uses Gini Impurity as the \n",
    "# measure to base the split of. The idea of Gini Impurity is that if we stop the classifier\n",
    "# after splitting by that attribute, What would be the probability of a false label \n",
    "# You can change the \"criterion\" used by the classifier to use information gain instead\n",
    "# clf=tree.DecisionTreeClassifier(criterion=\"entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our Decision tree has now been created. To see it represented visually, we'll use a \n",
    "# software called graphviz. You can download and install the graphviz application. Using\n",
    "# sci-kit learn we can export our tree to a file with .dot extension which will open up in \n",
    "# graphviz. If you open the .dot file in a plain text editor - it will look very much like \n",
    "# a markup language like xml\n",
    "\n",
    "with open(\"titanic.dot\",\"w\") as f:\n",
    "    f = tree.export_graphviz(clf,\n",
    "                            feature_names=features,out_file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AS we saw , the tree created is huge, it keeps going until it cannot split the training \n",
    "# data any further. It's very hard to make sense of what is going on in this tree, and \n",
    "# it's most likely overfitted.\n",
    "# The DecisionTreeClassifier() has many attributes you can use to control the tree\n",
    "# max_depth : The max allowed depth of the tree\n",
    "# max_leaf_nodes : The max number of leaf nodes allowed \n",
    "# min_samples_split: The minimum number of samples to perform a split, if a subset has lesser\n",
    "# samples, it's made a leaf node with whichever label has the max count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20344711,  0.51252297,  0.1479089 ,  0.0539263 ,  0.08219471])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tree visually tells you which are the more important features. The classifier has an \n",
    "# attribute which tells us the relative importance of features using a metric known as \n",
    "# Gini Importance \n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see how this tree performs when you run it on the test data from Kaggle. \n",
    "# We'll use our classifier to predict whether the passenger survived. Then we'll submit\n",
    "# the results and see how it goes ! \n",
    "def transformTestDataTitanic(testFile,features):\n",
    "    # We'll do a similar transformation on the test data, ie pick the specified features and\n",
    "    # map categorical variables to numerical variables \n",
    "    # In this case we need to keep track of the passenger ids so we can write them back to the \n",
    "    # csv that we'll submit to Kaggle\n",
    "    transformData=[]\n",
    "    ids=[]\n",
    "    genderMap={\"male\":1,\"female\":2,\"\":\"\"}\n",
    "    embarkMap={\"C\":1,\"Q\":2,\"S\":3,\"\":\"\"}\n",
    "    blank=\"\"\n",
    "    with open(testFile,\"r\") as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        lineNum=1\n",
    "        for row in lineReader:\n",
    "            if lineNum==1:\n",
    "                header=row\n",
    "            else: \n",
    "                allFeatures=list(map(lambda x:genderMap[x] if row.index(x)==3 else embarkMap[x] \n",
    "                               if row.index(x)==10 else x,row))\n",
    "                featureVector=[allFeatures[header.index(feature)] for feature in features]\n",
    "                # note that the test csv does not contain a column with the labels (Survived (1/0))\n",
    "                # We'll have to deal with blanks a little differently here\n",
    "                # We can't just ignore all the passengers with missing values as the Kaggle \n",
    "                # submission needs you to predict the outcome even for passengers whose attribute data\n",
    "                # is missing\n",
    "                # So, for now let's use a default value whenever we encounter a blank\n",
    "                featureVector=list(map(lambda x:0 if x==\"\" else x, featureVector))\n",
    "                transformData.append(featureVector)\n",
    "                ids.append(row[0])\n",
    "            lineNum=lineNum+1 \n",
    "    return transformData,ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def titanicTest(classifier,resultFile,transformDataFunction=transformTestDataTitanic):\n",
    "    # This function will take our classifier and run it on the test data\n",
    "    testFile=\"/Users/swethakolalapudi/Desktop/Kaggle/Titanic/test.csv\"\n",
    "    testData=transformDataFunction(testFile,features)#call the function we just wrote\n",
    "    result=classifier.predict(testData[0])\n",
    "    with open(resultFile,\"w\") as f:\n",
    "        ids=testData[1]\n",
    "        lineWriter=csv.writer(f,delimiter=',',quotechar=\"\\\"\")\n",
    "        lineWriter.writerow([\"PassengerId\",\"Survived\"])#The submission file needs to have a header\n",
    "        for rowNum in range(len(ids)):\n",
    "            try:\n",
    "                lineWriter.writerow([ids[rowNum],result[rowNum]])\n",
    "            except(Exception,e):\n",
    "                print(e)\n",
    "\n",
    "# Let's take this for a spin! \n",
    "resultFile=\"/Users/swethakolalapudi/Desktop/Kaggle/Titanic/result1.csv\"\n",
    "titanicTest(clf,resultFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So our current strategy is bettern than predicting that every passenger has died. But not by much. \n",
    "# One of the reasons this performs so badly is because of the way we dealt with the missing values \n",
    "# Let's now use a slightly more intelligent strategy to replace the missing values\n",
    "def transformTestDataTitanicv2(testFile,features):\n",
    "    # We'll do a similar transformation on the test data, ie pick the specified features and\n",
    "    # map categorical variables to numerical variables \n",
    "    # In this case we need to keep track of the passenger ids so we can write them back to the \n",
    "    # csv that we'll submit to Kaggle\n",
    "    transformData=[]\n",
    "    ids=[]\n",
    "    # This time we'll map each missing value with the most common value or the average value of that variable\n",
    "    genderMap={\"male\":1,\"female\":2,\"\":1} # Map blanks to males\n",
    "    embarkMap={\"C\":1,\"Q\":2,\"S\":3,\"\":3} # Map the default port of embarkation to Southampton\n",
    "    blank=\"\"\n",
    "    with open(testFile,\"r\") as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        lineNum=1\n",
    "        for row in lineReader:\n",
    "            if lineNum==1:\n",
    "                header=row\n",
    "            else: \n",
    "                allFeatures=list(map(lambda x:genderMap[x] if row.index(x)==3 else embarkMap[x] \n",
    "                               if row.index(x)==10 else x,row))\n",
    "                \n",
    "                # note that the test csv does not contain a column with the labels (Survived (1/0))\n",
    "                # We'll have to deal with blanks a little differently here\n",
    "                # We can't just ignore all the passengers with missing values as the Kaggle \n",
    "                # submission needs you to predict the outcome even for passengers whose attribute data\n",
    "                # is missing\n",
    "                # So, for now let's use a default value whenever we encounter a blank\n",
    "                #featureVector=map(lambda x:0 if x==\"\" else x, featureVector)\n",
    "                \n",
    "                # The second column is Passenger class, let the default value be 2nd class\n",
    "                if allFeatures[1]==\"\":\n",
    "                    allFeatures[1]=2\n",
    "                # Let the default age be 30\n",
    "                if allFeatures[4]==\"\":\n",
    "                    allFeatures[4]=30\n",
    "                # Let the default number of companions be 0 (assume if we have no info, the passenger\n",
    "                # was travelling alone)\n",
    "                if allFeatures[5]==\"\":\n",
    "                    allFeatures[5]=0\n",
    "                # By eyeballing the data , the average fare seems to be around 30\n",
    "                if allFeatures[8]==\"\":\n",
    "                    allFeatures[8]=32\n",
    "                featureVector=[allFeatures[header.index(feature)] for feature in features]     \n",
    "                transformData.append(featureVector)\n",
    "                ids.append(row[0])\n",
    "            lineNum=lineNum+1 \n",
    "    return transformData,ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's get a new result with this new function \n",
    "resultFile=\"/Users/swethakolalapudi/Desktop/Kaggle/Titanic/result3.csv\"\n",
    "titanicTest(clf,resultFile,transformTestDataTitanicv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll now do 2 things\n",
    "# We'll write some code to perform cross-validation. We'll use this to test the performance of a DecisionTreeClassifier\n",
    "# and a Randomforest classifier and see which one gives better results. \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "folds=2\n",
    "# We are going to use 2-fold cross validation. In 2-fold cross validation there are 3 steps\n",
    "\n",
    "# 1. Divide the training data set into 2 equal parts randomly, Let these be D0 and D1\n",
    "# 2. Let the test data be D0 and the training data be D1 . Train the model and compute the accuracy\n",
    "# 3. Let the test data be D1 and the training data be D0. Train the model and compute the accuracy\n",
    "\n",
    "# If you are using k fold validation, you would divide the data into k equal folds and then make each \n",
    "# of the folds the test data in turn. \n",
    "\n",
    "size = len(trainingData[0])\n",
    "#Remember: trainingData was a tuple returned by our function that read the titanic csv file. The first member of \n",
    "# the tuple was the transformed list of feature vectors. The second member was the list of corresponding labels \n",
    "# Survived (0/1)\n",
    "\n",
    "data=trainingData[0]\n",
    "labels=trainingData[1]\n",
    "\n",
    "# Now we'll do step 1 ie divide the data into 2 (or k) equal parts. k= number of folds. \n",
    "# We have to divide the training data randomly. In order to do so we'll set up a list - each member of this \n",
    "# list will the fold number to which the corresponding member of the training data will be assigned. This list will \n",
    "# be generated randomly ie each member of the list will be chosen randomly from the integers 1 to k. Each of the numbers\n",
    "# between 1 to k will have an equal probability of being chosen. At the end, we'll have a list in which number of 1s\n",
    "# = number of 2s = number of 3s .... \n",
    "\n",
    "# We'll use a random number generator from numpy for this. \n",
    "datafold=list(np.random.random_integers(1,folds,size))\n",
    "\n",
    "# This will give us a list of length \"size\" , in which each number is chosen from list [1,2,3...folds], with probability\n",
    "# being equal for any number being chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datafold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafold.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafold.count(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The counts are not perfectly equal, but we'll live with it for now. \n",
    "# We'll use this list to divide our data into 2 equal parts. Each of those parts will in turn become the \n",
    "# test data set. Let's use a loop for this \n",
    "from sklearn import tree\n",
    "accuracy=[]# This will hold the accuracy of the model for each fold. \n",
    "for i in range(folds):\n",
    "    # range(folds) will give us a list which starts from 0 and goes to folds-1\n",
    "    testDataCV=[data[sample] for sample in range(size) if datafold[sample]==i+1]\n",
    "    # in the first iteration i=0, i+1 = 1. This line will take each element in the list data , if the corresponding\n",
    "    # element in datafold=1, it will add that element to the testDataCV list. \n",
    "    testLabelsCV=[labels[sample] for sample in range(size) if datafold[sample]==i+1]\n",
    "    trainingDataCV=[data[sample] for sample in range(size) if datafold[sample]<>i+1]\n",
    "    trainingLabelsCV=[labels[sample] for sample in range(size) if datafold[sample]<>i+1]\n",
    "    testDataSize=len(testDataCV)\n",
    "    # We're ready to train a classifier on the training data, then we'll predict on the test data and \n",
    "    # compute the accuracy. \n",
    "    clf=tree.DecisionTreeClassifier(max_depth=10)\n",
    "    X=np.array(trainingDataCV)\n",
    "    y=np.array(trainingLabelsCV)\n",
    "    clf=clf.fit(X,y)\n",
    "    result=list(clf.predict(testDataCV))\n",
    "    # result will be a list of 0s and 1s , we'll compare it to the labels list for the test data and compute\n",
    "    # the accuracy\n",
    "    foldAccuracy=float(sum(map(lambda x:1 if result[x]==testLabelsCV[x] else 0, range(testDataSize))))*100/float(testDataSize)\n",
    "    accuracy.append(foldAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78.14207650273224, 75.72254335260115]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy # Accuracy with Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's now do the same but with a Random Forest \n",
    "# The counts are not perfectly equal, but we'll live with it for now. \n",
    "# We'll use this list to divide our data into 2 equal parts. Each of those parts will in turn become the \n",
    "# test data set. Let's use a loop for this \n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "accuracy=[]# This will hold the accuracy of the model for each fold. \n",
    "for i in range(folds):\n",
    "    # range(folds) will give us a list which starts from 0 and goes to folds-1\n",
    "    testDataCV=[data[sample] for sample in range(size) if datafold[sample]==i+1]\n",
    "    # in the first iteration i=0, i+1 = 1. This line will take each element in the list data , if the corresponding\n",
    "    # element in datafold=1, it will add that element to the testDataCV list. \n",
    "    testLabelsCV=[labels[sample] for sample in range(size) if datafold[sample]==i+1]\n",
    "    trainingDataCV=[data[sample] for sample in range(size) if datafold[sample]<>i+1]\n",
    "    trainingLabelsCV=[labels[sample] for sample in range(size) if datafold[sample]<>i+1]\n",
    "    testDataSize=len(testDataCV)\n",
    "    # We're ready to train a classifier on the training data, then we'll predict on the test data and \n",
    "    # compute the accuracy. \n",
    "    clf=RandomForestClassifier(n_estimators=1000)\n",
    "    X=np.array(trainingDataCV)\n",
    "    y=np.array(trainingLabelsCV)\n",
    "    clf=clf.fit(X,y)\n",
    "    result=list(clf.predict(testDataCV))\n",
    "    # result will be a list of 0s and 1s , we'll compare it to the labels list for the test data and compute\n",
    "    # the accuracy\n",
    "    foldAccuracy=float(sum(map(lambda x:1 if result[x]==testLabelsCV[x] else 0, range(testDataSize))))*100/float(testDataSize)\n",
    "    accuracy.append(foldAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80.32786885245902, 80.92485549132948]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
