{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ecdb5c71208b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;31m# Now to get a URL and summarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;31m######################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'urllib2'"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################################################################\n",
    "# THis example is pretty much entirely based on this excellent blog post\n",
    "# http://glowingpython.blogspot.in/2014/09/text-summarization-with-nltk.html\n",
    "# Thanks to TheGlowingPython, the good soul that wrote this excellent article!\n",
    "# That blog is is really interesting btw.\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# nltk - \"natural language toolkit\" is a python library with support for \n",
    "#         natural language processing. Super-handy.\n",
    "# Specifically, we will use 2 functions from nltk\n",
    "#  sent_tokenize: given a group of text, tokenize (split) it into sentences\n",
    "#  word_tokenize: given a group of text, tokenize (split) it into words\n",
    "#  stopwords.words('english') to find and ignored very common words ('I', 'the',...) \n",
    "#  to use stopwords, you need to have run nltk.download() first - one-off setup\n",
    "######################################################################################\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "######################################################################################\n",
    "# We have use dictionaries so far, but now that we have covered classes - this is a good\n",
    "# time to introduce defaultdict. THis is class that inherits from dictionary, but has\n",
    "# one additional nice feature: Usually, a Python dictionary throws a KeyError if you try \n",
    "# to get an item with a key that is not currently in the dictionary. \n",
    "# The defaultdict in contrast will simply create any items that you try to access \n",
    "# (provided of course they do not exist yet). To create such a \"default\" item, it relies \n",
    "# a function that is passed in..more below. \n",
    "######################################################################################\n",
    "from collections import defaultdict\n",
    "\n",
    "######################################################################################\n",
    "#  punctuation to ignore punctuation symbols\n",
    "######################################################################################\n",
    "from string import punctuation\n",
    "\n",
    "######################################################################################\n",
    "# heapq.nlargest is a function that given a list, easily and quickly returns\n",
    "# the 'n' largest elements in the list. More below\n",
    "######################################################################################\n",
    "from heapq import nlargest\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# Our first class, named FrequencySummarizer \n",
    "######################################################################################\n",
    "class FrequencySummarizer:\n",
    "    # indentation changes - we are now inside the class definition\n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        # The constructor named __init__\n",
    "        # THis function will be called each time an object of this class is \n",
    "        # instantiated\n",
    "        # btw, note how the special keyword 'self' is passed in as the first\n",
    "        # argument to each method (member function).\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut \n",
    "        # Words that have a frequency term lower than min_cut \n",
    "        # or higer than max_cut will be ignored.\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "        # Punctuation symbols and stopwords (common words like 'an','the' etc) are ignored\n",
    "        #\n",
    "        # Here self._min_cut, self._max_cut and self._stopwords are all member variables\n",
    "        # i.e. each object (instance) of this class will have an independent version of these\n",
    "        # variables. \n",
    "        # Note how this function is used to set up the member variables to their appropriate values\n",
    "    # indentation changes - we are out of the constructor (member function, but we are still inside)\n",
    "    # the class.\n",
    "    # One important note: if you are used to programming in Java or C#: if you define a variable here\n",
    "    # i.e. outside a member function but inside the class - it becomes a STATIC member variable\n",
    "    # THis is an important difference from Java, C# (where all member variables would be defined here)\n",
    "    # and is a common gotcha to be avoided.\n",
    "\n",
    "    def _compute_frequencies(self, word_sent):\n",
    "        # next method (member function) which takes in self (the special keyword for this same object)\n",
    "        # as well as a list of sentences, and outputs a dictionary, where the keys are words, and\n",
    "        # values are the frequencies of those words in the set of sentences\n",
    "        freq = defaultdict(int)\n",
    "        # defaultdict, which we referred to above - is a class that inherits from dictionary,\n",
    "        # with one difference: Usually, a Python dictionary throws a KeyError if you try \n",
    "        # to get an item with a key that is not currently in the dictionary. \n",
    "        # The defaultdict in contrast will simply create any items that you try to access \n",
    "        # (provided of course they do not exist yet). THe 'int' passed in as argument tells\n",
    "        # the defaultdict object to create a default value of 0\n",
    "        for s in word_sent:\n",
    "        # indentation changes - we are inside the for loop, for each sentence\n",
    "          for word in s:\n",
    "            # indentation changes again - this is an inner for loop, once per each word_sent\n",
    "            # in that sentence\n",
    "            if word not in self._stopwords:\n",
    "                # if the word is in the member variable (dictionary) self._stopwords, then ignore it,\n",
    "                # else increment the frequency. Had the dictionary freq been a regular dictionary (not a \n",
    "                # defaultdict, we would have had to first check whether this word is in the dict\n",
    "                freq[word] += 1\n",
    "        # Done with the frequency calculation - now go through our frequency list and do 2 things\n",
    "        #   normalize the frequencies by dividing each by the highest frequency (this allows us to \n",
    "        #            always have frequencies between 0 and 1, which makes comparing them easy\n",
    "        #   filter out frequencies that are too high or too low. A trick that yields better results.\n",
    "        m = float(max(freq.values()))\n",
    "        # get the highest frequency of any word in the list of words\n",
    "        for w in freq.keys():\n",
    "            # indentation changes - we are inside the for loop\n",
    "            freq[w] = freq[w]/m\n",
    "            # divide each frequency by that max value, so it is now between 0 and 1\n",
    "            if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "                # indentation changes - we are inside the if statement - if we are here the word is either\n",
    "                # really common or really uncommon. In either case - delete it from our dictionary\n",
    "                del freq[w]\n",
    "                # remember that del can be used to remove a key-value pair from the dictionary\n",
    "        return freq\n",
    "        # return the frequency list\n",
    "\n",
    "    def summarize(self, text, n):\n",
    "        # next method (member function) which takes in self (the special keyword for this same object)\n",
    "        # as well as the raw text, and the number of sentences we wish the summary to contain. Return the \n",
    "        # summary\n",
    "        sents = sent_tokenize(text)\n",
    "        # split the text into sentences\n",
    "        assert n <= len(sents)\n",
    "        # assert is a way of making sure a condition holds true, else an exception is thrown. Used to do \n",
    "        # sanity checks like making sure the summary is shorter than the original article.\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "        # This 1 sentence does a lot: it converts each sentence to lower-case, then \n",
    "        # splits each sentence into words, then takes all of those lists (1 per sentence)\n",
    "        # and mushes them into 1 big list\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        # make a call to the method (member function) _compute_frequencies, and places that in\n",
    "        # the member variable _freq. \n",
    "        ranking = defaultdict(int)\n",
    "        # create an empty dictionary (of the superior defaultdict variety) to hold the rankings of the \n",
    "            # sentences. \n",
    "        for i,sent in enumerate(word_sent):\n",
    "            # Indentation changes - we are inside the for loop. Oh! and this is a different type of for loop\n",
    "            # A new built-in function, enumerate(), will make certain loops a bit clearer. enumerate(sequence), \n",
    "            # will return (0, thing[0]), (1, thing[1]), (2, thing[2]), and so forth.\n",
    "            # A common idiom to change every element of a list looks like this:\n",
    "            #  for i in range(len(L)):\n",
    "            #    item = L[i]\n",
    "            #    ... compute some result based on item ...\n",
    "            #    L[i] = result\n",
    "            # This can be rewritten using enumerate() as:\n",
    "            # for i, item in enumerate(L):\n",
    "            #    ... compute some result based on item ...\n",
    "            #    L[i] = result\n",
    "            for w in sent:\n",
    "                # for each word in this sentence\n",
    "                if w in self._freq:\n",
    "                    # if this is not a stopword (common word), add the frequency of that word \n",
    "                    # to the weightage assigned to that sentence \n",
    "                    ranking[i] += self._freq[w]\n",
    "        # OK - we are outside the for loop and now have rankings for all the sentences\n",
    "        sents_idx = nlargest(n, ranking, key=ranking.get)\n",
    "        # we want to return the first n sentences with highest ranking, use the nlargest function to do so\n",
    "        # this function needs to know how to get the list of values to rank, so give it a function - simply the \n",
    "        # get method of the dictionary\n",
    "        return [sents[j] for j in sents_idx]\n",
    "       # return a list with these values in a list\n",
    "# Indentation changes - we are done with our FrequencySummarizer class!\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# Now to get a URL and summarize\n",
    "######################################################################################\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "######################################################################################\n",
    "# Introducing Beautiful Soup: \" Beautiful Soup is a Python library for pulling data out of \n",
    "# HTML and XML files. It works with your favorite parser to provide idiomatic ways of \n",
    "# navigating, searching, and modifying the parse tree. It commonly saves programmers hours \n",
    "# or days of work.\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def get_only_text_washington_post_url(url):\n",
    "    # This function takes in a URL as an argument, and returns only the text of the article in that URL. \n",
    "    page = urllib2.urlopen(url).read().decode('utf8')\n",
    "    # download the URL\n",
    "    soup = BeautifulSoup(page)\n",
    "    # initialise a BeautifulSoup object with the text of that URL\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n",
    "    # use this code to get everything in that text that lies between a pair of \n",
    "    # <article> and </article> tags. We do this because we know that the URLs we are currently\n",
    "    # interested in - those from the WashingtonPost have this nice property\n",
    "\n",
    "    # OK - we got everything between the <article> and </article> tags, but that everything\n",
    "    # includes a bunch of other stuff we don't want\n",
    "    # Now - repeat, but this time we will only take what lies between <p> and </p> tags\n",
    "    # these are HTML tags for \"paragraph\" i.e. this should give us the actual text of the article\n",
    "    soup2 = BeautifulSoup(text)\n",
    "    if soup2.find_all('p')!=[]:\n",
    "        text = ' '.join(map(lambda p: p.text, soup2.find_all('p')))\n",
    "    # use this code to get everything in that text that lies between a pair of \n",
    "    # <p> and </p> tags. We do this because we know that the URLs we are currently\n",
    "    # interested in - those from the WashingtonPost have this nice property\n",
    "    return soup.title.text, text\n",
    "# Return a pair of values (article title, article body)\n",
    "# Btw note that BeautifulSoup return the title without our doing anything special - \n",
    "# this is why BeautifulSoup works so much better than say regular expressions at parsing HTML\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "# OK! Now lets give this code a spin\n",
    "#####################################################################################\n",
    "someUrl = \"https://www.washingtonpost.com/news/the-switch/wp/2015/08/06/why-kids-are-meeting-more-strangers-online-than-ever-before/\"\n",
    "# the article we would like to summarize\n",
    "textOfUrl = get_only_text_washington_post_url(someUrl)\n",
    "# get the title and text\n",
    "fs = FrequencySummarizer()\n",
    "# instantiate our FrequencySummarizer class and get an object of this class\n",
    "summary = fs.summarize(textOfUrl[1], 3)\n",
    "# get a summary of this article that is 3 sentences long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "someUrl = \"https://www.washingtonpost.com/local/does-your-office-have-a-mouse-or-rat-problem-of-course-it-does/2016/03/12/46ccac24-e61c-11e5-b0fd-073d5930a7b7_story.html?hpid=hp_hp-more-top-stories_no-name%3Ahomepage%2Fstory\"\n",
    "textOfUrl = get_only_text_washington_post_url(someUrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'   Multitask1021  The majority of teenagers don\\'t consider meeting strangers online a taboo, with six in 10 saying they have met\\xa0at least\\xa0one new friend on the Web. Teens are also texting and communicating through online games and social networks more frequently\\xa0than they are spending time together in person. And of those who\\xa0meet people online, one-third also followed up with an in-person meeting. These findings are part of a new in-depth\\xa0study from the Pew Research Center aimed at understanding how online interactions are shaping the social lives and identities of American teens. Broadly speaking, the research found that the line between the virtual and real worlds has almost completely blurred -- and that kids say they have deep and meaningful relationships with people online and in person. \"The digital world has taken its place alongside school and friends\\' houses and extracurriculars as a place where teens go to make and strengthen friendships,\" said Amanda Lenhart, author of the report \"Teens, Technology & Friendships\" and an associate\\xa0director of research at Pew. \"Like it or not, this is where our teens talk, plot, laugh and fight with some of the most important\\xa0people in their lives.\" The current\\xa0generation of digital natives, who hit adolescence just as the iPhone and Facebook took off in popularity, are charting new\\xa0territory. And while the implications of online social activity among teens is not yet clear, some parents and child development experts warn that the intensity of online interactions is presenting\\xa0new pressures for youth and their parents. Researchers note how teens worry about missing out on conversations on social media, focus with increasing intensity on their online appearance, and react to the amplifying effect of social media. One cruel remark, teens who took the Pew survey said, can feel devastating in front of a big audiences and anxiety abounds over pressure to maintain gleaming reputations. \"Young people are very aware that people have highly curated images and that text fights can quickly go out of control and they are trying to sort it all out,\" said Rosalind Wiseman, author of \"Queen Bees and Wannabes\" and speaker on youth issues. \"But adults have to respect that these are deep relationships that play out online, and we can\\'t dismiss them.\" The abundance of technology has fueled the jump in and complexity of online interactions. Three-quarters of teens have a smartphone. A majority\\xa0have a social media account and play some sort of online game. According to the survey of 1,060 teens ages 13 to 17, texting remains the most popular form of digital communication. More than half of\\xa0teens say they text friends every day, and three-quarters do at least every few days. Only 25 percent of teens say they are able to meet with friends in person outside of school hours every day. Even with their closest friends, teens are spending as much time at each other\\'s homes as they are on social networks or gaming platforms. And those digital spaces are far more popular than hanging out with friends at coffee shops or the mall, the survey shows. For boys, online gaming has become\\xa0the center of social activity, with 84 percent of teen males playing video games compared to 59 percent of girls. Boys are more frequently making new friends online with six in 10 saying they\\'ve befriended a stranger via games or other apps. For 40 percent of these boys, the first piece of information they share online is their gaming handle. For Chasion Adams, 17, gaming friends are as valuable as the friends he has made at his D.C. high school and in his neighborhood. When he gets home from school, he powers up his Playstation and for the next few hours he\\'ll play games such as \"Call of Duty\" against friends from around the world. There is constant banter, with a lot of smack talk and joking. \"It\\'s like we have the game in common which is cool and then we talk about everything else like what kind of music we like and what\\'s going on with school,\" he said. Over a few years, he\\'s grown to know much about their family and personal lives. Chasion even met one friend in person after they discovered they were only a one-hour drive apart. When he is offline, Adams communicates with his gamer friends on social media apps such as Kik, Twitter and Instagram. According to the Pew survey, 53 percent of teens say they feel more connected to people after they play games with them. And the vast majority of teens -- particularly boys -- say they feel relaxed and happy connecting\\xa0with people while playing games. Most teens also check in daily to social media\\xa0platforms such as Twitter and Facebook. Girls, more than boys, use these sites to stay connected with friends. Yet the feelings toward social media are more ambivalent. Nearly nine in 10 teens say they\\'ve witnessed people over-share information about themselves on social media. Nearly half of those surveyed say they\\'ve at least occasionally seen posts about events that they were invited to; and\\xa085 percent said they think social media users present a carefully crafted image of themselves online that may not be authentic. But the vast majority also say they social media allows them to stay better connected to friends\\' feelings and events in their lives. It can be particularly hard for younger teens who feel pressure to create images of themselves on social media just as they are grappling with their budding identities. When Chloe Becker first used Facebook, Snapchat and Instagram as a young teenager, she lacked confidence and was sometimes overwhelmed. One comment could invite a pile-on of replies - sometimes the remarks were harsh or hurtful. At times, total strangers reached out to her. But\\xa0she also discovered a benefit to\\xa0the social network interactions\\xa0-- she could carefully consider her responses and choose what she shared. It was in some ways easier than face-to-face interaction. \"Authenticity is a tricky thing on social media because I had friends who were very friendly on Facebook but not in person, Becker said. \"But when you are young what does it mean to be authentic? You are trying so many different things.\" Now, heading off to college in Austin and with much more confidence, the 18-year-old often meets new friends on Facebook. The aspiring computer engineer joined a page for hackathons and others interested in coding. Like Becker, many say their friendship are completely entwined online and off. On a recent sweltering\\xa0August evening, Jessie Kinney and Megan Oliver took refuge at Tysons Corner. The two, wearing matching neon yellow tank tops, have been friends since their fresman year at Yorktown High School in Arlington.\\xa0Immediately after they met, they friended each other on Facebook so their interactions have\\xa0always been half\\xa0in person and\\xa0half through\\xa0texting and social media. Even when Megan went to Greece for two weeks earlier in the summer, the two were in close daily contact. But another big life change could change the dynamic. They are heading off to different colleges. Jessie will go to West Virginia University. Megan will be attending High Point University in North Carolina. Meeting up at Tysons a few days before they separate, they said\\xa0they are confident they will stay in close contact. \"I\\'m sure we\\'ll be really busy,\" Jessie said. \"But I\\'ll see her on Facebook and Instagram.\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textOfUrl[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
